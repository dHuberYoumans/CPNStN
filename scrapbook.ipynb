{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from scipy.special import binom\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from analysis import bootstrap\n",
    "\n",
    "from typing import Callable\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "* [wrapped normal distributions](https://mpmath.org/doc/0.19/functions/elliptic.html#jacobi-theta-functions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformation Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Recall that the following three spaces are all diffeomorphic \n",
    "\n",
    "$$Q = \\{ z \\in \\mathbb{C}^{n+1} \\mid \\sum_k z_k^2 = 1 \\}$$\n",
    "\n",
    "$$TS^n = \\{ (x,y) \\mid x \\in S^n, \\langle x , y \\rangle = 0\\}$$\n",
    "\n",
    "$$S^n \\times S^n / \\Delta = \\{(u,v) \\mid u,v \\in S^n, u \\neq v \\}$$\n",
    "\n",
    "The diffeomorphism are given by \n",
    "\n",
    "$$\\Psi \\colon TS^n \\to Q \\quad , \\quad (x,y) \\mapsto (x \\sqrt{1 + \\lVert y \\rVert^2} + i y)$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\Phi \\colon S^n \\times S^n / \\Delta \\to TS^n \\quad , \\quad (u,v) \\mapsto \\left(u, \\frac{v - \\langle u, v\\rangle u}{1 - \\langle u, v\\rangle}\\right)$$\n",
    "\n",
    "### The Model\n",
    "\n",
    "For $z,w$ in $\\mathbb{CP}^n$: \n",
    "\n",
    "$$S(z,w) = -\\beta \\sum_{a,b} z_a \\bar z_b w_b \\bar w_a$$\n",
    "\n",
    "Now, forget about the $U(1)$ invariance (can be integrated out since compact) and fix the non-compact part of the $\\mathbb{C}^*$ gauge group via \n",
    "\n",
    "$$\\delta( \\lvert z \\rvert^2 - 1)  \\delta( \\lvert w \\rvert^2 - 1)$$ \n",
    "\n",
    "with \n",
    "\n",
    "$$\\lvert z \\rvert^2 = \\sum_a \\lvert z_a \\rvert^2$$\n",
    "\n",
    "Rewrite it in real variables: let $z_a = x_a + i y_a$, $w_a = u_a + i v_a$ and define _real_ vectors $Z = (\\dots,x_a,y_a,\\dots)$ and $W = (\\dots,u_a,v_a,\\dots)$ with $\\lVert Z \\rVert^2 = 1$ and likewise for $W$.\n",
    "\n",
    "Define $h = id + J$ where $J = diag(\\sigma_y, \\dots, \\sigma_y)$ where \n",
    "\n",
    "$$\\sigma_y = \\begin{pmatrix} 0 & -i \\\\ i & 0\\end{pmatrix}$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$S(Z,W) = \\beta \\langle Z , T W \\rangle \\langle W , T Z \\rangle$$\n",
    "\n",
    "where $\\langle \\dots \\rangle$ is the standard inner product in $\\mathbb{R}^{2n+2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIFFEOMORPHISMS TS^n <-> Q <-> S^n x S^n / Delta\n",
    "\n",
    "def Psi(p): # TS^n -> Q\n",
    "    x, y = p # unpack point (into base point x, tangent vector y)\n",
    "\n",
    "    z = x*np.sqrt(1 + np.inner(y,y)) + y*1j\n",
    "\n",
    "    return z\n",
    "\n",
    "def invPsi(z): # Q -> TS^n\n",
    "    re, im = (z.real,z.imag) # seperate into re and im part\n",
    "    \n",
    "    x = re / np.sqrt(1 + np.inner(im,im))\n",
    "    y = im\n",
    "    \n",
    "    return (x,y)\n",
    "\n",
    "def Phi(p): # S^n x S^n / Delta -> TS^n\n",
    "    u, v = p # unpack p\n",
    "\n",
    "    x = u # base point\n",
    "    y = (v - np.inner(u,v)*u) / (1 - np.inner(u,v)) # tangent vector \n",
    "\n",
    "    return (x,y)\n",
    "\n",
    "def invPhi(p): # TS^n -> S^n x S^n / Delta\n",
    "    x,y = p # unpack base point, tangent vector\n",
    "\n",
    "    u = x\n",
    "    v = (x*(np.inner(y,y) - 1) + 2*y ) / (np.inner(y,y) + 1)\n",
    "\n",
    "    return u, v\n",
    "\n",
    "\n",
    "# HERMITIAN INNER PRODUCT \n",
    "\n",
    "def h(n:int,Z:np.ndarray,W:np.ndarray) -> np.ndarray:\n",
    "    \"\"\" \n",
    "    Quadratic form h used in action functional \n",
    "\n",
    "    :param n: complex dimension of CP^n\n",
    "    :type n: int\n",
    "\n",
    "    :param Z: (samples,2n + 2,1) real column vector \n",
    "    :type Z: np.ndarray\n",
    "\n",
    "    :param W: (samples,2n + 2,1) real column vector \n",
    "    :type W: np.ndarray\n",
    "\n",
    "    :returns: h(Z,W) for each sample\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "    id = np.eye(2*(n + 1))\n",
    "\n",
    "    sigma_y = np.array([[0,-1j],[1j,0]])\n",
    "\n",
    "    J = block_diag(*[sigma_y for _ in range(n+1)])\n",
    "\n",
    "    # result = np.inner(Z.flatten(), ((id + J) @ W).flatten())\n",
    "    result = ( Z.transpose(0,-1,1) @ ( (id + J) @ W) ).flatten()\n",
    "\n",
    "    return result\n",
    "\n",
    "def Scmplx(z:complex,w:complex,beta:float) -> float:\n",
    "    return  - beta * np.inner(z,w.conjugate()) * np.inner(w, z.conjugate())\n",
    "\n",
    "# TOY MODEL ACTION FUNCTIONAL\n",
    "\n",
    "def S(n:int, Z:np.ndarray,W:np.ndarray,beta:float) -> float: # action on real variables\n",
    "    return  - beta * h(n,Z,W) * h(n,W,Z)\n",
    "\n",
    "# 2X2 ROTATION MATRIX FOR SAMPLING SPHERE\n",
    "def R(t:float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    2x2 roation matrix \n",
    "\n",
    "    :param t: rotation angle\n",
    "    :type t: float\n",
    "\n",
    "    :returns: rotation matrix np.array([[cos(t),-sin(t)],[sin(t),cos(t)]])\n",
    "    :rtype: np.ndarray\n",
    "    \"\"\"\n",
    "\n",
    "    c = np.cos(t)\n",
    "    s = np.sin(t)\n",
    "\n",
    "    return np.array([[c,-s],[s,c]])\n",
    "\n",
    "# LINEAR ALGEBRA\n",
    "\n",
    "def inner(a,X): # inner product <X,a>\n",
    "    \"\"\"\n",
    "    shape X,a = (samples, 2n + 2, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert X.shape == a.shape\n",
    "    assert len(X.shape) == 3\n",
    "    assert X.shape[-1] == 1\n",
    "\n",
    "    return X.transpose(0,-1,1) @ a\n",
    "\n",
    "def outer(a,X): # outer product \n",
    "    \"\"\"\n",
    "    shape X,a = (samples, 2n + 2, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert X.shape == a.shape\n",
    "    assert len(X.shape) == 3\n",
    "    assert X.shape[-1] == 1\n",
    "\n",
    "    return a * X.transpose(0,-1,1) \n",
    "\n",
    "def proj(a,X): # projection onto X\n",
    "\n",
    "    inner_Xa = inner(X,a)\n",
    "    \n",
    "    return inner_Xa * X / inner(X,X)\n",
    "\n",
    "def proj_perp(a,X): # proj perpendicular \n",
    "    return a - proj(a,X)\n",
    "\n",
    "# JACOBIAN TOY MODEL\n",
    "\n",
    "def M(a,X):\n",
    "    \"\"\"\n",
    "    shape X,a = (samples, 2n + 2, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    assert X.shape == a.shape\n",
    "    assert len(X.shape) == 3\n",
    "    assert X.shape[-1] == 1\n",
    "\n",
    "    id = np.eye(X.shape[1])\n",
    "\n",
    "    inner_Xa = inner(X,a)\n",
    "    inner_XX = inner(X,X)\n",
    "\n",
    "    return  - id * inner_Xa  - outer(a,X) / inner_XX +  2*inner_Xa * outer(X,X) / inner_XX**2\n",
    "\n",
    "def Jac(a,X):\n",
    "\n",
    "    m = X.shape[1] # = 2n + 2\n",
    "\n",
    "    Y = proj_perp(a,X)\n",
    "\n",
    "    lam = np.sqrt(1 + inner(Y,Y)) # shape (samples,1,1)\n",
    "\n",
    "    id = np.eye(X.shape[1])\n",
    "\n",
    "    MM = M(a,X)\n",
    "\n",
    "    J = id*lam + outer( (MM @ Y), X) / lam + 1j*MM\n",
    "\n",
    "    det = np.linalg.det(J) # shape (samples,)\n",
    "\n",
    "    return det / lam.flatten()**2 #(lam - 1j * inner(a,X))**(m-1)  # WHY DONT WE HAVE TO RETURN det / lam.flatten()**2 ? the extra factor should come from the delta function constraint\n",
    "\n",
    "# COMPLEXIFICATION \n",
    "\n",
    "def tildeZ(a,X): # paramerisation of (analytical continuation of) Z\n",
    "\n",
    "    Y = proj_perp(a,X)\n",
    "\n",
    "    return X * np.sqrt(1 + inner(Y,Y)) + 1j*Y\n",
    "\n",
    "# MCMC\n",
    "\n",
    "def sweep_sphere(phi:list,pairs:list,f) -> int:  \n",
    "    \"\"\"\n",
    "    sweeping the sphere\n",
    "\n",
    "    :param n: complex dimension CP^n\n",
    "    :type n: int\n",
    "    \"\"\"\n",
    "\n",
    "    alpha = 0 # number accepted\n",
    "\n",
    "    # LOOP OVER COMPONENTS \n",
    "    for pair in pairs:\n",
    "        a,b = pair # unpack indices\n",
    "        phi_old = phi[-1] # current state \n",
    "\n",
    "        # SAMPLE 2x2 ROTATION ANGLE\n",
    "        theta = np.random.normal(loc=0,scale=1)\n",
    "\n",
    "        # ROTATE COMPONENT\n",
    "        v = np.array([phi_old[a],phi_old[b]])\n",
    "        vnew = R(theta) @ v\n",
    "\n",
    "        # PROPOSE NEW STATE\n",
    "        phi_new = phi_old.copy()\n",
    " \n",
    "        phi_new[a] = vnew[0]\n",
    "        phi_new[b] = vnew[1]\n",
    "    \n",
    "        # ACCEPTENCE PROBABILITIES\n",
    "        A = np.minimum(1, f(phi_new[np.newaxis,:,:]) / (f(phi_old[np.newaxis,:,:])) ) # newaxis since f takes list of vectors: shape = (smaples,2n+2,1)\n",
    "\n",
    "        # CHECK IF ACCEPTED\n",
    "        p = np.random.uniform(low=0,high=1) # draw vector of uniform rnds\n",
    "        \n",
    "        if p <= A : # accept if p < A\n",
    "            phi.append(phi_new)\n",
    "            alpha += 1\n",
    "        else:\n",
    "            phi.append(phi_old)\n",
    "\n",
    "    return alpha / len(pairs)\n",
    "\n",
    "\n",
    "def MCMC_toy(n:int, Z0:complex,W0:complex,beta:float,N_steps:int,burnin:int,k:int) -> tuple[np.ndarray,float,np.ndarray]:\n",
    "    \"\"\"\n",
    "    :param n: complex dimension CP^n \n",
    "    :type n: int\n",
    "    \"\"\"\n",
    "\n",
    "    N = 2*n + 2 # real dimension (before quotient by C^*)\n",
    "    pairs = list(combinations(np.arange(N),2)) # indices of pairs to rotate\n",
    "\n",
    "    # SETUP\n",
    "    Z = [Z0] \n",
    "    W = [W0]\n",
    "\n",
    "    alpha = 0 # number accepted\n",
    "\n",
    "    expSw = lambda z: np.exp( -S(n,z,W[-1][np.newaxis,:,:],beta) ) # w fixed\n",
    "    expSz = lambda w: np.exp( -S(n,Z[-1][np.newaxis,:,:],w,beta) ) # z fixed\n",
    "\n",
    "    for _ in tqdm(range(N_steps)): # tqdm for progress bar\n",
    "\n",
    "        alpha += sweep_sphere(phi=Z,pairs=pairs,f=expSw) \n",
    "        alpha += sweep_sphere(phi=W,pairs=pairs,f=expSz)\n",
    "       \n",
    "    samples_Z, samples_W = (np.array(Z)[burnin::k], np.array(W)[burnin::k])\n",
    "\n",
    "    acception_rate = alpha / (2*N_steps) \n",
    "\n",
    "    return (samples_Z, samples_W, acception_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undeformed Path Integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "# INITIAL POINTS ON SPHERE\n",
    "Z0 = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "Z0 /= np.linalg.norm(Z0)\n",
    "\n",
    "W0 = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "W0 /= np.linalg.norm(W0)\n",
    "\n",
    "N_steps = 10_000\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "obs = []\n",
    "\n",
    "Nbeta = 10\n",
    "\n",
    "for beta in range(1,Nbeta):\n",
    "\n",
    "    Z_samples, W_samples, alpha = MCMC_toy(n=n,Z0=Z0,W0=W0,beta=beta,N_steps=N_steps,burnin=200,k=5)\n",
    "    obs.append(- S(n,Z_samples,W_samples,beta).real.mean()/beta)\n",
    "\n",
    "    print(f'{beta}/{Nbeta}',end='\\r')\n",
    "\n",
    "\n",
    "print(f'running time: {time.time() - t0:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = range(1,Nbeta)\n",
    "plt.plot(beta,obs,'o-')\n",
    "plt.xlabel(f'$\\\\beta$')\n",
    "plt.ylabel(f'$E[S / \\\\beta]$')\n",
    "plt.title(f'{n = } , {N_steps = :_d}, {alpha = :.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idea: \n",
    "\n",
    "1. deform $S^{2n + 1} \\to Q$ via $Z \\to \\tilde Z = Z + i \\zeta_a(Z)$  \n",
    "\n",
    "\n",
    "2. parametrize $Q$ via $TS^{2n+1}$:\n",
    "\n",
    "   $$Q \\ni \\tilde Z(X) = X \\sqrt{1 + \\lVert Y_a(X) \\rVert^2} + i Y_a(X)$$\n",
    "\n",
    "   where $X \\in S^{2n +1 }$ and \n",
    "   \n",
    "   $$Y(X,a) = \\frac{a - \\langle a, X \\rangle X}{1 - \\langle a, X \\rangle} \\in T_X S^{2n + 1} \\quad , \\quad a \\in \\mathbb{R}^{2n + 2}$$\n",
    "\n",
    "3. compute \n",
    "\n",
    "   $$\\langle \\mathcal{O}(Z) \\rangle _Q = \\langle \\det J(X) \\mathcal{O}(\\tilde Z(X)) e^{- (S(\\tilde Z(X)) - S(X))}\\rangle _{e^{-S(X)}}$$\n",
    "\n",
    "\n",
    "\n",
    "To do:\n",
    "\n",
    "1. choose $a \\in \\mathbb{R}^{2n + 1}$ fixed constant\n",
    "\n",
    "2. compute $Y_a(X)$ \n",
    "\n",
    "3. compute $\\lambda(a,X) = \\sqrt{1 + \\lVert Y_a(X) \\rVert^2}$\n",
    "\n",
    "4. compute $J_{ij} = \\lambda \\delta_{ij} + \\frac{(M Y)_i X_j}{\\lambda} + i M_{ij}$ where\n",
    "\n",
    "$$M_{ij} = \\partial_{X_i}(Y_a(X))_j = \\frac{a_i(a_j - X_j) - \\delta_{ij} \\langle a, X\\rangle (1 - \\langle a, X\\rangle )}{(1 - \\langle a, X\\rangle)^2}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_one(X_samples,W_samples,beta):\n",
    "    n = (X_samples.shape[1] - 2 ) // 2\n",
    "\n",
    "    a = np.ones_like(X_samples)\n",
    "\n",
    "    tildeZ_samples = tildeZ(a,X_samples)\n",
    "\n",
    "    obs = ( Jac(a,X_samples) * np.exp( - ( S(n,tildeZ_samples,W_samples,beta) - S(n,X_samples,W_samples,beta) ) ) )\n",
    "\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "# SETUP OBSERVABLE\n",
    "beta = 1\n",
    "fuzzy_one_mean = lambda samples: fuzzy_one(X_samples=samples,W_samples=W_samples,beta=beta).mean()\n",
    "\n",
    "ones = []\n",
    "err = []\n",
    "\n",
    "# INITIAL POINTS ON SPHERE\n",
    "X0 = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "X0 /= np.linalg.norm(X0)\n",
    "\n",
    "W0 = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "W0 /= np.linalg.norm(W0)\n",
    "\n",
    "\n",
    "# SETUP MONTE CARLO\n",
    "N_steps = 5_000\n",
    "burnin = 200\n",
    "skip = 5\n",
    "\n",
    "# SETUP ERRORS (BOOTSTRAP)\n",
    "Nboot = 200\n",
    "\n",
    "# TIME \n",
    "t0 = time.time()\n",
    "\n",
    "for _ in range(50):\n",
    "    X_samples, W_samples, alpha = MCMC_toy(n=n,Z0=X0,W0=W0,beta=beta,N_steps=N_steps,burnin=burnin,k=skip)\n",
    "\n",
    "    # a = np.random.normal(size=X_samples.shape) #np.ones_like(X_samples)\n",
    "    # tildeZ_samples = tildeZ(a,X_samples)\n",
    "    # obs = Jac(a,X_samples) * np.exp( - ( S(n,tildeZ_samples,W_samples,beta) - S(n,X_samples,W_samples,beta) ) )\n",
    "    # ones.append(obs.real.mean())\n",
    "    # err.append(obs.std() / np.sqrt(len(X_samples)))\n",
    "\n",
    "    ones.append(fuzzy_one_mean(X_samples))\n",
    "    err.append(bootstrap(X_samples,Nboot=Nboot,f=fuzzy_one_mean)[1])\n",
    "    \n",
    "\n",
    "print(f'running time: {time.time() - t0:.2f}s\\n')\n",
    "\n",
    "# print(f'{alpha = }\\n')\n",
    "# print(f'{obs.mean() = } \\n')\n",
    "# print(f'{obs.std() / np.sqrt(len(X_samples)) = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_ones = [one.real for one in ones]\n",
    "\n",
    "t = np.arange(len(real_ones))\n",
    "\n",
    "fig = plt.figure(figsize=(15,4))\n",
    "\n",
    "plt.errorbar(t, real_ones, yerr=err, fmt='o', capsize=5, label=\"Data with Error\")\n",
    "plt.hlines(y=1.0,xmin=-1,xmax=len(real_ones),colors='red',ls='--')\n",
    "plt.title(f'fuzzy ones: {n= }, {N_steps = }, {Nboot = }')\n",
    "plt.ylabel(r'$\\langle 1 \\rangle_{\\text{deformed}}$')\n",
    "plt.xlabel('runs');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad Descent Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINDING NULLSTELLEN - PEDESTRIAN WAY\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "x = torch.tensor(1.5,requires_grad=True) # initial guess\n",
    "\n",
    "lr = 0.1 # learning rate\n",
    "\n",
    "epochs = 20 # nb optimization steps\n",
    "\n",
    "print(f'{x = }')\n",
    "\n",
    "# OPTIMIZATION \n",
    "for epoch in range(epochs):\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        x -= lr * x.grad\n",
    "        print(f'{x = } \\t {x.grad = }')\n",
    "    x.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINDING NULLSTELLEN - MODEL\n",
    "\n",
    "class Nullstellen(nn.Module):\n",
    "    def __init__(self,f,x0:float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.f = f # function whose zeros we want to find \n",
    "        self.x = nn.Parameter( torch.tensor([x0]) )\n",
    "\n",
    "    def forward(self):\n",
    "        return self.f(self.x)\n",
    "\n",
    "# HYPERPARAMETER\n",
    "alpha = 0.01 # learning rate\n",
    "eps = 1e-8 # stopping cond \n",
    "next = True\n",
    "x0 = np.random.randn()\n",
    "convergence_steps = 0\n",
    "\n",
    "# MODEL\n",
    "model = Nullstellen(f=lambda x: x**2,x0=x0)\n",
    "optimizer = optim.SGD(model.parameters(),lr=alpha)\n",
    "print(f'init:\\n{model.x}')\n",
    "\n",
    "# RUN\n",
    "while next:\n",
    "    convergence_steps += 1\n",
    "    y = model()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    if y < eps:\n",
    "        next = False\n",
    "\n",
    "print(f'\\nfinal:\\n{model.x}')\n",
    "print(f'\\n{convergence_steps = }')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE SAMPLES\n",
    "n = 2 # cmplx dim\n",
    "\n",
    "# SETUP OBSERVABLE\n",
    "beta = 1\n",
    "\n",
    "# INITIAL POINTS ON SPHERE\n",
    "x0 = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "x0 /= np.linalg.norm(x0)\n",
    "\n",
    "y0 = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "y0 /= np.linalg.norm(y0)\n",
    "\n",
    "\n",
    "# SETUP MONTE CARLO\n",
    "N_steps = 5_000\n",
    "burnin = 200\n",
    "skip = 5\n",
    "\n",
    "x, y, alpha = MCMC_toy(n=n,Z0=x0,W0=y0,beta=beta,N_steps=N_steps,burnin=burnin,k=skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A(phi,beta):\n",
    "    \"\"\"\n",
    "    phi = X, Y \n",
    "    \"\"\"\n",
    "    N = phi.shape[-2]\n",
    "    n = (N - 2) // 2 # n in CP^n\n",
    "\n",
    "    # PUT CST OUTSIDE FCT CALL\n",
    "    id = torch.eye(N)\n",
    "\n",
    "    sigma_y = np.array([[0,-1j],[1j,0]]) # 2nd Pauli\n",
    "\n",
    "    T = id + torch.tensor( block_diag(*[sigma_y for _ in range(n+1)]) ) \n",
    "    # END CST\n",
    "\n",
    "    X = phi[:,0]\n",
    "    Y = phi[:,1]\n",
    "\n",
    "    hXY = ( X.transpose(-1,-2) @ (T @ Y) ).flatten()\n",
    "\n",
    "    hYX =  ( Y.transpose(-1,-2) @ (T @ X) ).flatten()\n",
    "\n",
    "    return - beta * hXY * hYX\n",
    "    \n",
    "def fuzzy_one(a,phi,beta):\n",
    "    X = phi\n",
    "    N = x.shape[1] # 2n + 2\n",
    "\n",
    "    # ALGEBRA\n",
    "    inner_aX = a.transpose(-1,-2) @ X\n",
    "    outer_aX = a @ X.transpose(-1,-2)\n",
    "\n",
    "    inner_XX = X.transpose(-1,-2) @ X\n",
    "    outer_XX = X @ X.transpose(-1,-2)\n",
    "\n",
    "    Y = a - inner_aX / inner_XX * X # P^perp(a,X)\n",
    "\n",
    "    inner_YY = Y.transpose(-1,-2) @ Y\n",
    "\n",
    "    id = torch.eye(N)\n",
    "\n",
    "    # ACTION FUNCTIONAL\n",
    "\n",
    "    A0 = A(X,beta)\n",
    "\n",
    "\n",
    "    # COMPLEXIFICATION \n",
    "\n",
    "    tildeX = X * torch.sqrt(1 + inner_YY) + 1j*Y\n",
    "\n",
    "    Atilde = A(tildeX,beta)\n",
    "\n",
    "    # JACOBIAN\n",
    "\n",
    "    lam = torch.sqrt(1 + inner_YY) # shape (samples,2,1,1), torch.tensor\n",
    "\n",
    "    M = - id * inner_aX / inner_XX - outer_aX / inner_XX +  2*inner_aX * outer_XX / inner_XX**2\n",
    "\n",
    "    J = id*lam + (M @ Y) @ X.transpose(-1,-2) / lam + 1j*M\n",
    "\n",
    "    det = torch.det(J) # shape (samples, #particles) -> need to multiply\n",
    "    detJ = det / lam.squeeze(dim=(-1,-2))**2 \n",
    "\n",
    "    detJ = torch.prod(detJ,dim=-1) # shape (samples,)\n",
    "\n",
    "    assert len(detJ.shape) == 1, f'detJ has wrong dim: {detJ.shape} but must be 1' \n",
    "\n",
    "\n",
    "    OO = ( detJ * torch.exp( - ( Atilde - A0 ) ) ) # observable \n",
    "\n",
    "    return OO\n",
    "\n",
    "def real2cmplx(phi):\n",
    "    # phi.shape = (... , N, 1)\n",
    "\n",
    "    z = phi[...,::2,:] + 1j*phi[...,1::2,:]\n",
    "    zbar = phi[...,::2,:] - 1j*phi[...,1::2,:]\n",
    "\n",
    "    return (z,zbar)\n",
    "\n",
    "def two_pt_fct(phi):\n",
    "    zeta, zetabar = real2cmplx(phi)\n",
    "\n",
    "    z = zeta[:,0]\n",
    "    zbar = zetabar[:,0]\n",
    "    w = zeta[:,1]\n",
    "    wbar = zetabar[:,1]\n",
    "\n",
    "    return (z[:,0]*wbar[:,0]*w[:,1]*zbar[:,1]).squeeze(dim=-1) \n",
    "\n",
    "def one_pt_fct(phi):\n",
    "    zeta, zetabar = real2cmplx(phi)\n",
    "\n",
    "    z = zeta[:,0]\n",
    "    zbar = zetabar[:,0]\n",
    "    \n",
    "    return (z[:,0]*zbar[:,1]).squeeze(dim=-1) \n",
    "\n",
    "def def_obs(obs):\n",
    "    \n",
    "    def compute(a,phi,beta):\n",
    "        X = phi\n",
    "        N = x.shape[1] # 2n + 2\n",
    "\n",
    "        # ALGEBRA\n",
    "        inner_aX = a.transpose(-1,-2) @ X\n",
    "        outer_aX = a @ X.transpose(-1,-2)\n",
    "\n",
    "        inner_XX = X.transpose(-1,-2) @ X\n",
    "        outer_XX = X @ X.transpose(-1,-2)\n",
    "\n",
    "        Y = a - inner_aX / inner_XX * X # P^perp(a,X)\n",
    "\n",
    "        inner_YY = Y.transpose(-1,-2) @ Y\n",
    "\n",
    "        id = torch.eye(N)\n",
    "\n",
    "        # ACTION FUNCTIONAL\n",
    "\n",
    "        A0 = A(X,beta)\n",
    "\n",
    "\n",
    "        # COMPLEXIFICATION \n",
    "\n",
    "        tildeX = X * torch.sqrt(1 + inner_YY) + 1j*Y\n",
    "\n",
    "        Atilde = A(tildeX,beta)\n",
    "\n",
    "        # JACOBIAN\n",
    "\n",
    "        lam = torch.sqrt(1 + inner_YY) # shape (samples,2,1,1), torch.tensor\n",
    "\n",
    "        M = - id * inner_aX / inner_XX - outer_aX / inner_XX +  2*inner_aX * outer_XX / inner_XX**2\n",
    "\n",
    "        J = id*lam + (M @ Y) @ X.transpose(-1,-2) / lam + 1j*M\n",
    "\n",
    "        det = torch.det(J) # shape (samples, #particles) -> need to multiply\n",
    "        detJ = det / lam.squeeze(dim=(-1,-2))**2 \n",
    "\n",
    "        detJ = torch.prod(detJ,dim=-1) # shape (samples,)\n",
    "\n",
    "        assert len(detJ.shape) == 1, f'detJ has wrong dim: {detJ.shape} but must be 1' \n",
    "\n",
    "\n",
    "        OO = (obs(tildeX) * detJ * torch.exp( - ( Atilde - A0 ) ) ) # observable \n",
    "\n",
    "        return OO\n",
    "    \n",
    "    return compute\n",
    "\n",
    "def grab(x):\n",
    "    return x.detach().cpu().numpy()\n",
    "\n",
    "def loss(obs):\n",
    "    return obs.var()\n",
    "\n",
    "def logloss(obs):\n",
    "    return torch.log(obs.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDG OPTIMIZATION \n",
    "class DefObs(nn.Module):\n",
    "    def __init__(self,obs,a,beta):\n",
    "        \"\"\"\n",
    "    \n",
    "        obs = observable\n",
    "        a,b = (contour) deformation parameters \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.a = nn.Parameter(a) # deformation parameter(s)\n",
    "        self.beta = beta\n",
    "        self.obs = obs\n",
    "\n",
    "    def forward(self,samples):\n",
    "        a_ = self.a.cdouble().unsqueeze(0)\n",
    "\n",
    "        return self.obs(a_,samples,self.beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# p1 = torch.tensor(x,dtype=complex)\n",
    "# p2 = torch.tensor(y,dtype=complex)\n",
    "\n",
    "phi = torch.tensor(np.stack([x,y],axis=1),dtype=complex)\n",
    "\n",
    "a = 0.0*torch.randn(*phi.shape[1:]) # 2 deform params \n",
    "\n",
    "beta = 1.0\n",
    "alpha = 1e-3\n",
    "\n",
    "\n",
    "split = int(0.7*phi.shape[0])\n",
    "phi_train = phi[:split]\n",
    "phi_val = phi[split:]\n",
    "\n",
    "\n",
    "model = DefObs(obs=def_obs(one_pt_fct), a=a, beta=beta)\n",
    "optimizer = optim.Adam(model.parameters(),lr=alpha)\n",
    "\n",
    "\n",
    "anorm = []\n",
    "losses_train = []\n",
    "losses_val = []\n",
    "observable = []\n",
    "\n",
    "epochs = 1_000\n",
    "\n",
    "loss_fct = logloss\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# RUN\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    optimizer.zero_grad() # clear grads\n",
    "\n",
    "    # MINI-BATCHING\n",
    "    minibatch = np.random.randint(low=0,high=len(phi_train),size=batch_size)\n",
    "\n",
    "    # TRAIN\n",
    "    deformed_obs = model(samples=phi_train[minibatch])\n",
    "    loss_train =loss_fct(deformed_obs)\n",
    "    loss_train.backward()\n",
    "\n",
    "    # print(f'grad: {model.a.grad}\\n')\n",
    "    with torch.no_grad():\n",
    "        observable.append(grab(deformed_obs).mean())\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # VALIDATION\n",
    "    with torch.no_grad():\n",
    "        deformed_obs_val = model(samples=phi_val)\n",
    "        loss_val = loss_fct(deformed_obs_val)\n",
    "        \n",
    "        \n",
    "    \n",
    "    losses_train.append(grab(loss_train))\n",
    "    losses_val.append(grab(loss_val))\n",
    "    anorm.append(np.linalg.norm(grab(a).ravel()))\n",
    "\n",
    "        \n",
    "\n",
    "# PLOT\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1,ncols=3,figsize=(15,4))\n",
    "\n",
    "ax[0].plot(anorm)\n",
    "ax[0].set_title('norm a')\n",
    "ax[1].plot(losses_train,label='loss')\n",
    "ax[1].plot(losses_val,label='val_loss')\n",
    "ax[1].legend()\n",
    "ax[1].set_title('losses')\n",
    "ax[2].plot([z.real for z in observable],label='re')\n",
    "ax[2].plot([z.imag for z in observable],label='imag')\n",
    "ax[2].set_title('defromed obs');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notation:\n",
    "\n",
    "$n = \\dim_{\\mathbb{C}}$\n",
    "\n",
    "$N = 2n + 2 = \\dim_{\\mathbb{R}}$\n",
    "\n",
    "$z, w \\in \\mathbb{CP}^n \\cong \\mathbb{R}^{2n + 2} \\ni Z, W$\n",
    "\n",
    "parametrisation: $\\tilde Z = X \\sqrt{1 + \\lVert Y(X)\\rvert^2} + i Y(X)$ where $X \\in \\mathbb{R}^{2n + 2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "def grab(x):\n",
    "    return x.detach().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATING SAMPLES\n",
    "n = 2\n",
    "beta = 1\n",
    "N_steps = 1_000\n",
    "burnin = 100\n",
    "skip = 5\n",
    "\n",
    "phi0 = torch.randn(2,(2*n + 2),1)\n",
    "phi0 /= torch.linalg.vector_norm(phi0, dim=1, keepdim=True)\n",
    "\n",
    "phi, alpha = create_samples(n=n,phi0=phi0,beta=beta,N_steps=N_steps,burnin=burnin,k=skip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(phi, 'foo.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = torch.load('foo.dat',weights_only=True)\n",
    "print(type(foo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**!! Remarks !!** \n",
    "\n",
    "* should rename $a$ to $\\omega$.\n",
    "\n",
    "* It would be interesting to see if we could use more Lie theory, namely the rootspace decomposition! I'll try to work this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL (n=2) \n",
    "\n",
    "# SET HYPERPARAMETERS\n",
    "beta = 1.0\n",
    "alpha = 1e-4\n",
    "i,j = 2, 1 # parameter for fuzzy zero\n",
    "# obs = lambda phi: two_pt(phi,i)\n",
    "# obs = lambda phi: fuzzy_zero(phi,i,j)\n",
    "obs = fuzzy_one\n",
    "\n",
    "# DEFORMATION\n",
    "a0 = 0.1*torch.ones_like(phi[0])\n",
    "deformation = Linear()\n",
    "\n",
    "# LOSS\n",
    "loss_fct = loss\n",
    "loss_name = 'loss' if loss_fct == loss else 'logloss'\n",
    "\n",
    "# MODEL\n",
    "model = ZeroDModel(dim_C=n,deformation=deformation,a0=a0,observable=obs,beta=beta) # deformation_param=a\n",
    "\n",
    "# SET EPOCHS\n",
    "epochs = 1_000\n",
    "\n",
    "# TRAINING\n",
    "observable, observable_var, losses_train, losses_val, anorm, a0, af = train(model,phi,epochs=epochs,loss_fct=loss_fct,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODEL (n=2) \n",
    "\n",
    "# # SET HYPERPARAMETERS\n",
    "# beta = 1.0\n",
    "# alpha = 1e-4\n",
    "# i,j = 2, 1 # parameter for fuzzy zero\n",
    "# # obs = lambda phi: two_pt(phi,i)\n",
    "# obs = lambda phi: fuzzy_zero(phi,i,j)\n",
    "# # obs = fuzzy_one\n",
    "\n",
    "# # DEFORMATION\n",
    "# # deformation = 'homogeneous'\n",
    "# # a = None\n",
    "# a = torch.diag_embed(torch.tensor([[1j,-1j,0]]*2)) # su(n+1)\n",
    "# # a = torch.diag_embed(torch.tensor([[1j,0,0]]*2)) # u(n+1)\n",
    "# # a = rho(a) # don't have your param rho(a) but only a -> use rho in method\n",
    "\n",
    "# # SET LOSS FUNCTION\n",
    "# loss_fct = loss\n",
    "# loss_name = 'loss' if loss_fct == loss else 'logloss'\n",
    "\n",
    "# model = ZeroDModel(dim_C=n,observable=obs,deformation=deformation,a0=a0,beta=beta)\n",
    "\n",
    "# # SET EPOCHS\n",
    "# epochs = 1_000\n",
    "\n",
    "# observable, observable_var, losses_train, losses_val, anorm, a0, af = train(model,phi,epochs=epochs,loss_fct=loss_fct,)\n",
    "\n",
    "# undeformed_obs = obs(phi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIANCE PLOT\n",
    "fig, ax = plt.subplots(nrows=3,ncols=2,figsize=(15,10),gridspec_kw={'height_ratios': [1, 1, 2]})\n",
    "\n",
    "plt.suptitle(deformation + ' deformation\\n')\n",
    "ax[0,1].plot(anorm)\n",
    "ax[0,1].set_title('norm a')\n",
    "\n",
    "ax[0,0].plot(losses_train,label='loss')\n",
    "ax[0,0].plot(losses_val,label='val_loss')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set_title(loss_name)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot([z.real for z in observable],label='re')\n",
    "ax[1,0].plot([z.imag for z in observable],label='im',color='purple')\n",
    "ax[1,0].axhline(y=undeformed_obs.mean().real,xmin=0,xmax=phi.shape[0],label='OG re',color='red')\n",
    "ax[1,0].axhline(y=undeformed_obs.mean().imag,xmin=0,xmax=phi.shape[0],label='OG im',color='orange')\n",
    "ax[1,0].set_title('defromed obs')\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].plot([z.real for z in observable_var],label='deformed')\n",
    "ax[1,1].axhline(y=undeformed_obs.var(),xmin=0,xmax=phi.shape[0],label='undeformed',color='red')\n",
    "ax[1,1].set_title('obs variance')\n",
    "ax[1,1].legend()\n",
    "\n",
    "# if deformation == 'homogeneous' or deformation == 'torus':\n",
    "sns.heatmap(data=a0[0].imag,ax=ax[2,0],cmap='coolwarm')\n",
    "ax[2,0].set_title('deform param before training')\n",
    "sns.heatmap(data=af[0].imag,ax=ax[2,1],cmap='coolwarm')\n",
    "ax[2,1].set_title('deform param after training')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2pt function (toy model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *\n",
    "\n",
    "import analysis as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_toy_samples(n,beta,N_steps = 10_000, burnin = 1_000, skip = 10):\n",
    "    \"\"\" GENERATING SAMPLES \"\"\"\n",
    "\n",
    "    phi0 = torch.randn(2,(2*n + 2),1).double()\n",
    "    phi0 /= torch.linalg.vector_norm(phi0, dim=1, keepdim=True)\n",
    "\n",
    "    print(\"creating samples ... \\n\")\n",
    "\n",
    "    phi, alpha = create_samples_II(n,phi0=phi0,beta=beta,N_steps=N_steps,burnin=burnin,k=skip)\n",
    "\n",
    "    return phi, alpha\n",
    "\n",
    "def train_2pt(model,phi,epochs,loss_fct,lr=1e-4,split=0.7,batch_size=32):\n",
    "    \"\"\"\n",
    "    Training.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model: ZeroDModel\n",
    "        model to train\n",
    "\n",
    "    phi: torch.Tensor\n",
    "        MCMC samples \n",
    "\n",
    "    epochs: int\n",
    "        max number of epochs\n",
    "\n",
    "    loss_fct: Callable\n",
    "        loss function \n",
    "\n",
    "    lr: float, optional, default = 1e-4\n",
    "        learning rate\n",
    "\n",
    "    optimizer: torch.optim.Optimizer, optional, default = None \n",
    "        optimizer, if None then set to torch.optim.Adam\n",
    "\n",
    "    split: flaot, optional, default = 0.7\n",
    "        percentage of splitting into training and validation set\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "\n",
    "    observable: torch.Tensor\n",
    "        expectation value of the observable\n",
    "\n",
    "    observable_var: torch.Tensor\n",
    "        variance of the observable\n",
    "\n",
    "    losses_train: list[float]\n",
    "        losses of training set\n",
    "\n",
    "    losses_val: list[float]\n",
    "        losses of validation set\n",
    "\n",
    "    anorm: list[float]\n",
    "        vector/matrix norm of deformation parameter\n",
    "\n",
    "    a0: torch.Tensor\n",
    "        deformation parameter before training\n",
    "\n",
    "    af: torch.Tensor\n",
    "        deformation parameter after training\n",
    "    \"\"\"\n",
    "    # TRAIN-TEST SPLIT\n",
    "    split_ = int(split*phi.shape[0])\n",
    "    \n",
    "    phi_train = phi[:split_]\n",
    "    phi_val = phi[split_:]\n",
    "\n",
    "    # OPTIMIZER\n",
    "    optimizer_ = optim.Adam(model.parameters(),lr=lr)\n",
    "    \n",
    "    # MINI-BATCHING\n",
    "    batch_size_ = batch_size\n",
    "\n",
    "    observable = []\n",
    "    observable_var = []\n",
    "    losses_train = []\n",
    "    losses_val = []\n",
    "\n",
    "    # TRAINING\n",
    "    for epoch in tqdm.tqdm(range(epochs)):\n",
    "        optimizer_.zero_grad()\n",
    "\n",
    "        # MINI-BATCHING\n",
    "        minibatch = np.random.randint(low=0,high=len(phi_train),size=batch_size_)\n",
    "\n",
    "        # TRAIN\n",
    "        Otilde = model(phi_train[minibatch]) # already expectation value\n",
    "\n",
    "        loss_train = loss_fct(Otilde)\n",
    "        loss_train.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            observable.append(grab(Otilde).mean())\n",
    "\n",
    "            observable_var.append(grab(Otilde).mean())\n",
    "\n",
    "        optimizer_.step()\n",
    "\n",
    "        # VALIDATION\n",
    "        with torch.no_grad():\n",
    "            Otilde_val = model(phi_val) \n",
    "\n",
    "            loss_val = loss_fct(Otilde_val)\n",
    "            \n",
    "        losses_train.append(grab(loss_train))\n",
    "        losses_val.append(grab(loss_val))\n",
    "\n",
    "\n",
    "\n",
    "    return observable, observable_var, losses_train, losses_val\n",
    "\n",
    "\n",
    "    assert phi.shape[-2] == 2*n + 2, f\"phi has wrong (vector, real) dim. Expected {2*n + 2}, got {phi.shape[-2]}\"\n",
    "\n",
    "    X = phi[:,0]\n",
    "    XS = phi[:,1]\n",
    "\n",
    "    dtype = X.dtype # I don't like this!\n",
    "\n",
    "    a_ = rho(1j*su_n.embed(a[0])).to(dtype) # assuming Hermitian su(n) generators \n",
    "\n",
    "    outer_XX = X @ X.transpose(-1,-2)\n",
    "\n",
    "    # DEFORMATION / COMPLEXIFICATION\n",
    "    Y = a_ @ X \n",
    "    inner_YY = Y.transpose(-1,-2) @ Y\n",
    "\n",
    "    lam = torch.sqrt(1 + inner_YY)\n",
    "\n",
    "    tildeZ = X * lam + 1j*Y \n",
    "\n",
    "    tildeZ = torch.stack((X,XS),dim=1)\n",
    "    assert tildeZ.shape == phi.shape, \"tilde Z has wrong shape!\"\n",
    "\n",
    "    # JACOBIAN\n",
    "    J = identity*lam - outer_XX @ (a_ @ a_) / lam + 1j*a_\n",
    "    det = torch.det(J) # (samples, ) \n",
    "    print(f\"{J.shape = }\")\n",
    "    print(f\"{det.shape = }\")\n",
    "\n",
    "    detJ = det / (lam.squeeze(dim=(-1,-2))**2) # incl. extra factor from delta fct\n",
    "\n",
    "    # detJ = torch.prod(detJ,dim=-1) # total Jacobian (samples,)\n",
    "\n",
    "    assert len(detJ.shape) == 1, f'detJ has wrong dim: {detJ.shape} but must be 1' \n",
    "\n",
    "    return tildeZ, detJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi, alpha = generate_toy_samples(2,1.0,N_steps=100,burnin=0,skip=1)\n",
    "print(phi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "Nboot = 1000\n",
    "betas = [0.0, 0.5, 1.0, 2.0, 3.0, 5.0]\n",
    "undeformed_obs_re = []\n",
    "undeformed_obs_im = []\n",
    "\n",
    "for beta in betas:\n",
    "    phi, alpha = generate_toy_samples(n,beta)\n",
    "    obs_ = ToyObs.two_pt_full(phi)\n",
    "    mean_re, err_re = al.bootstrap(grab(obs_),Nboot=Nboot,f=al.rmean)\n",
    "    # mean_re, err_re = al.bootstrap(grab(obs_.real),Nboot=Nboot,f=al.rmean)\n",
    "    # mean_im, err_im = al.bootstrap(grab(obs_.imag),Nboot=Nboot,f=al.imean)\n",
    "    undeformed_obs_re.append((mean_re,err_re))\n",
    "    # undeformed_obs_im.append((mean_im,err_im))\n",
    "\n",
    "print(\"\\ndone\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_re = np.array([val for val, _ in undeformed_obs_re])\n",
    "err_obs_re = np.array([err for _, err in undeformed_obs_re])\n",
    "\n",
    "# obs_im = np.array([val for val, _ in undeformed_obs_im])\n",
    "# err_obs_im = np.array([err for _, err in undeformed_obs_im])\n",
    "\n",
    "plt.errorbar(betas,obs_re,err_obs_re,ecolor='red',barsabove=True,marker='o',ls='')\n",
    "plt.xlabel(r'$\\beta$')\n",
    "plt.ylabel('2-pt fn');\n",
    "\n",
    "# fig, ax = plt.subplots(nrows=1,ncols=2,figsize=(15,5))\n",
    "# ax[0].errorbar(betas,obs_re,err_obs_re,ecolor='red',barsabove=True,marker='o',ls='')\n",
    "# ax[0].set_xlabel(r'$\\beta$')\n",
    "# ax[0].set_ylabel('2-pt fn')\n",
    "# ax[0].set_title('real')\n",
    "\n",
    "# ax[1].errorbar(betas,obs_im,err_obs_im,ecolor='red',barsabove=True,marker='o',ls='')\n",
    "# ax[1].set_xlabel(r'$\\beta$')\n",
    "# ax[1].set_ylabel('2-pt fn')\n",
    "# ax[1].set_title('imag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(1)\n",
    "c = torch.cos(t)\n",
    "s = torch.sin(t)\n",
    "R = torch.stack([\n",
    "    torch.cos(t), -torch.sin(t),\n",
    "    torch.sin(t), torch.cos(t)\n",
    "], dim=-1).reshape(len(t), 2, 2)\n",
    "\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST COMPLEXIFY SINGLE FIELD\n",
    "n = 2\n",
    "beta = 1.0\n",
    "dim = n**2 + 2*n\n",
    "phi = torch.load(f\"samples_n{n}_b{beta}.dat\",weights_only=True) # n = 2 data\n",
    "a0 = 0.1*torch.randn(2,dim) # full hom\n",
    "deformation = Homogeneous(a0,n)\n",
    "\n",
    "tildeZ, detJ = deformation.complexify_single(phi)\n",
    "\n",
    "print(f\"1st field deformed? {np.allclose(grab(tildeZ[:,0] - phi[:,0]),torch.tensor(0.),atol=1e-12)}\")\n",
    "print(f\"2nd field undeformed? {np.allclose(grab(tildeZ[:,1] - phi[:,1]),torch.tensor(0.),atol=1e-12)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([torch.zeros(2,6,1),torch.randn(2,6,1)],dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING SAMPLES\n",
    "n= 2\n",
    "beta = 1.0\n",
    "\n",
    "phi_, _ = generate_toy_samples(n,beta,N_steps=1000,burnin=100,skip=10)\n",
    "\n",
    "print(f\"all vectors normalised? {torch.allclose((phi_.transpose(-1,-2) @ phi_),torch.tensor(1.,dtype=torch.double),atol=1e-6)}\")\n",
    "\n",
    "print(\"Testing norms of z, zbar\\n\")\n",
    "\n",
    "z, zbar = real2cmplx(phi)\n",
    "\n",
    "print(f\"{z.dtype = }\")\n",
    "\n",
    "print(f\"max norm of abs(z*zbar): {torch.abs((z*zbar).squeeze(-1)[:,0]).sum(dim=-1).max()}\")\n",
    "\n",
    "print(f\"all norms abs(z*zbar) are 1? {torch.allclose(torch.abs((z*zbar).squeeze(-1)[:,0]).sum(dim=-1),torch.tensor(1.0,dtype=torch.double),atol=1e-4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ SET HYPERPARAMETERS ################\n",
    "n = 2\n",
    "beta = 1.0\n",
    "\n",
    "alpha = 1e-3 # learning rate\n",
    "i,j = 0, 1 # parameter for fuzzy zero\n",
    "# obs = lambda phi: ToyObs.fuzzy_one(phi)\n",
    "# obs = lambda phi: ToyObs.one_pt(phi,i,j)\n",
    "obs = lambda phi: ToyObs.two_pt(phi,i,j)\n",
    "\n",
    "phi = torch.load(f\"samples_n{n}_b{beta}.dat\",weights_only=True) # n = 2 data\n",
    "\n",
    "S = lambda phi: ToyActionFunctional(n).action(phi,beta)\n",
    "\n",
    "# su(n+1)\n",
    "deformation_type = \"homogeneous\"\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "a0 = 0.1*torch.randn(2,dim) # full hom\n",
    "deformation = Homogeneous(a0,n)\n",
    "\n",
    "################ TRAINING ########################\n",
    "\n",
    "# LOSS\n",
    "loss_fct = loss\n",
    "loss_name = 'loss' if loss_fct == loss else 'logloss'\n",
    "\n",
    "# MODEL\n",
    "params = [S,deformation,obs,beta]\n",
    "model = CP(n,*params)\n",
    "\n",
    "# SET EPOCHS\n",
    "epochs = 2_000\n",
    "\n",
    "# TRAINING\n",
    "print(\"\\n training model ... \\n\")\n",
    "\n",
    "observable, observable_var, losses_train, losses_val = train_2pt(model,phi,epochs=epochs,loss_fct=loss_fct,batch_size=1024)\n",
    "\n",
    "undeformed_obs = obs(phi)\n",
    "\n",
    "print(\"\\n done.\")\n",
    "\n",
    "Nboot = 1_000\n",
    "mean_re, err_re = al.bootstrap(grab(undeformed_obs),Nboot=Nboot,f=al.rmean)\n",
    "mean_im, err_im = al.bootstrap(grab(undeformed_obs),Nboot=Nboot,f=al.imean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deformed_mean_re = torch.tensor(observable).real.mean().item()\n",
    "deformed_mean_im = torch.tensor(observable).imag.mean().item()\n",
    "\n",
    "print(f\"{mean_re = }\")\n",
    "print(f\"{mean_im = }\")\n",
    "print(f\"{deformed_mean_re = }\")\n",
    "print(f\"{deformed_mean_im = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = torch.tensor(0.)\n",
    "std = torch.tensor(2.)\n",
    "torch.normal(mu,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot([z.real for z in observable],label='re')\n",
    "plt.plot([z.imag for z in observable],label='im',color='purple')\n",
    "plt.axhline(y=mean_re,xmin=0,xmax=epochs,label='OG re',color='red')\n",
    "plt.axhline(y=mean_im,xmin=0,xmax=epochs,label='OG im',color='orange')\n",
    "plt.fill_between([0,epochs], [mean_re-err_re]*2, [mean_re+err_re]*2, alpha=0.5, color='red')\n",
    "plt.fill_between([0,epochs], [mean_im-err_im]*2, [mean_im+err_im]*2, alpha=0.5, color='orange')\n",
    "plt.title('2pt function')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "Delta = [z.real / mean_re for z in observable]\n",
    "plt.plot(Delta,label='re')\n",
    "plt.axhline(y=np.mean(Delta),xmin=0,xmax=epochs,color='red',label=r'mean $\\Delta$')\n",
    "# plt.plot([z.imag / mean_im for z in observable],label='im',color='purple')\n",
    "plt.title('quotient')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "Delta = [z.real - mean_re for z in observable]\n",
    "plt.plot(Delta,label='re')\n",
    "plt.axhline(y=np.mean(Delta),xmin=0,xmax=epochs,color='red',label=r'mean $\\Delta$')\n",
    "# plt.plot([z.imag / mean_im for z in observable],label='im',color='purple')\n",
    "plt.title('difference')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "rk = n-1\n",
    "dim = n**2 - 1\n",
    "su_n = LieSU(n+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = phi[0,0]\n",
    "a_ = rho(1j*su_n.embed(a0))[0]\n",
    "((foo @ foo.transpose(-1,-2)) @ a_ @ a_).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Basis $\\mathfrak{su}(n)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "rk = n-1\n",
    "dim = n**2 - 1\n",
    "su_n = LieSU(n)\n",
    "\n",
    "h1 = su_n.basis[0]\n",
    "h2 = su_n.basis[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{h1 = }\\n\\n\")\n",
    "print(f\"{h2 = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(h1 @ h2).trace().item() # not orthogonal!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = su_n.rnd_su()\n",
    "print(f\"{foo.shape = }\")\n",
    "print(f\"trace = {foo.trace().item()}\")\n",
    "print(f\"foo Hermitian: {torch.all(foo == foo.transpose(-1,-2).conj()) }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Torus Deformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ SET HYPERPARAMETERS ################\n",
    "n = 2\n",
    "beta = 1.0\n",
    "\n",
    "alpha = 1e-3 # learning rate\n",
    "i,j = 0, 2 # parameter for fuzzy zero\n",
    "# obs = fuzzy_one\n",
    "obs = lambda phi: ToyObs.one_pt(phi,i,j) # fuzzy_zero\n",
    "# obs = lambda phi: two_pt(phi,i,j)\n",
    "\n",
    "phi = torch.load(f\"samples_n{n}_b{beta}.dat\",weights_only=True) # n = 2 data\n",
    "\n",
    "S = lambda phi: ToyActionFunctional(n).action(phi,beta)\n",
    "\n",
    "# su(n+1)\n",
    "deformation_type = \"homogeneous\"\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "a0 = 0.1*torch.randn(2,dim) # full hom\n",
    "# a0 = torch.stack([torch.cat([0.1*torch.randn(rk),torch.zeros(dim-rk)]),torch.cat([0.1*torch.randn(rk),torch.zeros(dim-rk)])],dim=0) # torus\n",
    "deformation = Homogeneous(a0,n)\n",
    "\n",
    "################ TRAINING ########################\n",
    "\n",
    "# LOSS\n",
    "loss_fct = loss\n",
    "loss_name = 'loss' if loss_fct == loss else 'logloss'\n",
    "\n",
    "# MODEL\n",
    "params = [S,deformation,obs,beta]\n",
    "model = CP(n,*params)\n",
    "\n",
    "# SET EPOCHS\n",
    "epochs = 5_000\n",
    "\n",
    "# TRAINING\n",
    "print(\"\\n training model ... \\n\")\n",
    "\n",
    "observable, observable_var, losses_train, losses_val, anorm, a0, af = train(model,phi,epochs=epochs,loss_fct=loss_fct,batch_size=1024)\n",
    "\n",
    "print(\"\\n done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{(i,j) = }\\n\")\n",
    "\n",
    "print(f\"{torch.arcsinh(torch.tensor(a0[0])) = }\")\n",
    "print(f\"{torch.arcsinh(torch.tensor(af[0])) = }\")\n",
    "\n",
    "tau1 = torch.arcsinh(torch.tensor(af[0]))[0]\n",
    "tau2 = torch.arcsinh(torch.tensor(af[0]))[1]\n",
    "\n",
    "print(\"\")\n",
    "print(f\"tau1 = {tau1.item()}\")\n",
    "print(f\"tau2 = {tau2.item()}\")\n",
    "print(f\"tau1 > tau2 = {(tau1 > tau2).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Specific Torus Deformations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "beta = 1.0\n",
    "\n",
    "# SAMPLES\n",
    "phi = torch.load(f'samples_n{n}_b{beta:.1f}.dat',weights_only=True)\n",
    "\n",
    "# ACTION FUNCTIONAL\n",
    "S = lambda phi: ToyActionFunctional(n).action(phi,beta)\n",
    "\n",
    "# OBSERVABLE\n",
    "i,j = 0, 1\n",
    "obs = lambda phi: ToyObs.two_pt(phi,i,j)\n",
    "\n",
    "# su(n+1)\n",
    "deformation_type = \"homogeneous\"\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "a0 = 0.01*torch.randn(2,dim) # full hom\n",
    "\n",
    "deformation = Homogeneous(a0,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_tilde, detJ = deformation.complexify(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S(phi_tilde) - S(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ SET HYPERPARAMETERS ################\n",
    "n = 2\n",
    "beta = 1.0\n",
    "\n",
    "alpha = 1e-3 # learning rate\n",
    "i,j = 1, 2  # parameter for fuzzy zero\n",
    "# obs = fuzzy_one\n",
    "obs = lambda phi: ToyObs.one_pt(phi,i,j) # fuzzy_zero\n",
    "# obs = lambda phi: two_pt(phi,i,j)\n",
    "\n",
    "phi = torch.load(f\"samples_n{n}_b{beta:.1f}.dat\",weights_only=True) # n = 2 data\n",
    "\n",
    "S = lambda phi: ToyActionFunctional(n).action(phi,beta)\n",
    "\n",
    "# su(n+1)\n",
    "deformation_type = \"homogeneous\"\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "\n",
    "rres = []\n",
    "ires = []\n",
    "for k in range(11):\n",
    "    alpha1 = - np.sinh(k/10.)\n",
    "    alpha2 = 0.0 #- np.sinh(0.1*np.abs(np.random.randn()))\n",
    "    a0 = torch.stack([torch.cat([torch.tensor([alpha1,alpha2]),torch.zeros(dim-rk)]),torch.cat([0.1*torch.randn(rk),torch.zeros(dim-rk)])],dim=0) # torus\n",
    "    deformation = Homogeneous(a0,n)\n",
    "    params = [S,deformation,obs,beta]\n",
    "    model = CP(n,*params)\n",
    "    # O = model.obs(phi)\n",
    "    Otilde = model.deformed_obs(phi)\n",
    "    rvar = Otilde.real.var() #/ O.real.var()\n",
    "    ivar = Otilde.imag.var() #/ O.imag.var()\n",
    "    rres.append(grab(rvar))\n",
    "    ires.append(grab(ivar))\n",
    "\n",
    "plt.scatter(np.linspace(0,1,11),rres,label='real')\n",
    "plt.scatter(np.linspace(0,1,11),ires,label='imag',color='orange')\n",
    "plt.xlabel(r\"$\\tau_1$\")\n",
    "plt.ylabel(r\"$Var$\")\n",
    "plt.title(f\"$Var(O_{{{i}{j}}})$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Lattice with random data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "beta = 1.0\n",
    "\n",
    "N_samples = 100\n",
    "Lx = 8\n",
    "Ly = 8\n",
    "dim = 2*n + 2\n",
    "\n",
    "shape = (N_samples,Lx,Ly,dim,1)\n",
    "\n",
    "# RANDOM TEST SAMPLES\n",
    "phi = torch.randn(*shape)\n",
    "\n",
    "phi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING ACTION FUNCTIONAL\n",
    "S = lambda phi: LatticeActionFunctional(n).action(phi.cdouble(),beta)\n",
    "S(phi).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING HOMOGENEOUS DEFORMATION\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "a0 = 0.1*torch.randn(Lx,Ly,dim) # full hom\n",
    "deformation = Homogeneous(a0,n,mode=\"2D\")\n",
    "tildeZ, detJ = deformation.complexify(phi)\n",
    "print(f\"{tildeZ.shape = }\")\n",
    "print(f\"{detJ.shape = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING LINEAR DEFORMATION\n",
    "\n",
    "a0 = 0.1*torch.randn(phi[0].shape) \n",
    "deformation = Linear(a0,n,mode=\"2D\")\n",
    "tildeZ, detJ = deformation.complexify(phi)\n",
    "print(f\"{tildeZ.shape = }\")\n",
    "print(f\"{detJ.shape = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Lattice with actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *\n",
    "\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing running / exc times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 64\n",
    "beta = 4.5\n",
    "Nc = 3\n",
    "n = 2\n",
    "u = np.loadtxt(f'cpn_b{beta:.1f}_L{L}_Nc{Nc}_u.dat')[::10]\n",
    "n_cfg = len(u)\n",
    "ens = np.fromfile(f'cpn_b{beta:.1f}_L{L}_Nc{Nc}_ens.dat', dtype=np.complex128).reshape(n_cfg, L, L, Nc)\n",
    "phi = cmplx2real(torch.tensor(ens).unsqueeze(-1)).double()\n",
    "dim = n**2 + 2*n\n",
    "identity = torch.eye(2*n + 2)\n",
    "su_n = LieSU(n+1)\n",
    "a0 = torch.zeros(L,L,dim)\n",
    "\n",
    "\n",
    "X = phi\n",
    "dtype = X.dtype # I don't like this!\n",
    "\n",
    "with Timer(\"embedding\"):\n",
    "    a_ = rho(1j*su_n.embed(a0)).to(dtype) # assuming Hermitian su(n) generators \n",
    "\n",
    "with Timer(\"outer_XX\"):\n",
    "    outer_XX = X @ X.transpose(-1,-2)\n",
    "\n",
    "# DEFORMATION / COMPLEXIFICATION\n",
    "with Timer(\"Y\"):\n",
    "    Y = a_ @ X \n",
    "\n",
    "with Timer(\"inner_YY\"):\n",
    "    inner_YY = Y.transpose(-1,-2) @ Y\n",
    "\n",
    "with Timer(\"lam\"):\n",
    "    lam = torch.sqrt(1 + inner_YY)\n",
    "\n",
    "with Timer(\"tilde Z\"):\n",
    "    tildeZ = X * lam + 1j*Y \n",
    "\n",
    "with Timer(\"outer_XX @ a @ a\"):\n",
    "    outer_XX @ (a_ @ a_)\n",
    "\n",
    "with Timer(\"broadcasting outer_XX @ a @ a / lam\"):\n",
    "    outer_XX @ (a_ @ a_) / lam\n",
    "\n",
    "with Timer(\"J\"):\n",
    "    J = identity*lam - outer_XX @ (a_ @ a_) / lam + 1j*a_\n",
    "\n",
    "with Timer(\"J via einsum\"):\n",
    "    J_einsum = identity * lam - torch.einsum('...ij,...jk->...ik', outer_XX, torch.einsum('...ij,...jk-> ...ik', a_, a_)) / lam + 1j * a_\n",
    "\n",
    "with Timer(\"detJ\"):\n",
    "    detJ = torch.det(J)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing action values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 64\n",
    "beta = 4.5\n",
    "Nc = 3\n",
    "u = np.loadtxt(f'cpn_b{beta:.1f}_L{L}_Nc{Nc}_u.dat')[9::10]\n",
    "V = L**2\n",
    "S_tej = beta* V * u # if action is 0 centerd\n",
    "# S_tej =  - beta * (u - 2) * V # otherwise\n",
    "n_cfg = len(u)\n",
    "ens = np.fromfile(f'cpn_b{beta:.1f}_L{L}_Nc{Nc}_ens.dat', dtype=np.complex128).reshape(n_cfg, L, L, Nc)\n",
    "\n",
    "# prepping\n",
    "n=2\n",
    "S = lambda phi: LatticeActionFunctional(n).action_centered(phi.cdouble(),beta)\n",
    "phi = cmplx2real(torch.tensor(ens).unsqueeze(-1)).double()\n",
    "\n",
    "print(f\"check any lattice point has norm 1: {torch.allclose(torch.linalg.norm(phi.squeeze(-1),dim=-1),torch.tensor(1.0,dtype=torch.double))}\")\n",
    "\n",
    "S_donny = S(phi).real\n",
    "u_donny = LatticeActionFunctional(n).u(phi.cdouble()).real\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(grab(S_donny) - S_tej);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING T\n",
    "n = 2\n",
    "sigma_y = torch.tensor([[0,1j],[-1j,0]]) # VS overall - sign! To discuss\n",
    "T = (torch.eye(2*n + 2) + torch.block_diag(*[sigma_y for _ in range(n+1)])).cdouble()\n",
    "\n",
    "z = ens[0,0,0]\n",
    "w = ens[0,0,1]\n",
    "x = z.real\n",
    "y = z.imag\n",
    "u = w.real\n",
    "v = w.imag\n",
    "\n",
    "X1 = torch.tensor(list(zip(x,y)),dtype=torch.cdouble).flatten().view(-1,1)\n",
    "X2 = torch.tensor(list(zip(u,v)),dtype=torch.cdouble).flatten().view(-1,1)\n",
    "\n",
    "XX1 = cmplx2real(torch.tensor(z).unsqueeze(-1)).cdouble()\n",
    "XX2 = cmplx2real(torch.tensor(w).unsqueeze(-1)).cdouble()\n",
    "\n",
    "\n",
    "resT = (X1.transpose(-1,-2) @ (T @ X2)).squeeze(-1,-2)\n",
    "\n",
    "foo = (XX1.transpose(-1,-2) @ (T @ XX2)).squeeze(-1,-2)\n",
    "\n",
    "res = torch.tensor(np.sum(z.conjugate() * w))\n",
    "\n",
    "print(f\"{(resT - foo).item() = }\")\n",
    "print(f\"{(res - foo).item() = }\")\n",
    "print(f\"{(resT - res).item() = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "\n",
    "# Tej's code\n",
    "def inner(z1, z2):\n",
    "    return np.sum(np.conj(z1) * z2, axis=-1)\n",
    "\n",
    "def action(z):\n",
    "    latt_shape,  N = z.shape[:-1], z.shape[-1]\n",
    "    S = 0\n",
    "    for mu in range(len(latt_shape)):\n",
    "        z_fwd = np.roll(z, -1, axis=mu)\n",
    "        S += np.sum(np.abs(inner(z_fwd, z))**2 - 1)\n",
    "    return - beta * S\n",
    "\n",
    "S_tej_py = []\n",
    "for i in range(len(ens)):\n",
    "    S_tej_py.append(action(ens[i]))\n",
    "\n",
    "S_tej_py = torch.tensor(S_tej_py)\n",
    "\n",
    "################################################################################################################################################\n",
    "\n",
    "def action_donny(phi):\n",
    "    n = 2\n",
    "    sigma_y = np.array([[0,1j],[-1j,0]]) \n",
    "    T = (np.eye(2*n + 2) + block_diag(*[sigma_y for _ in range(n+1)]))\n",
    "    \n",
    "    S = 0\n",
    "    for mu in range(2):\n",
    "        z_fwd = (np.transpose(phi,axes=(0,1,3,2)) @ (T @ np.roll(phi,-1,axis=mu))).flatten()\n",
    "        zbar_fwd = (np.transpose(np.roll(phi,-1,axis=mu),axes=(0,1,3,2)) @ (T @ phi)).flatten()\n",
    "\n",
    "        S += np.sum(z_fwd * zbar_fwd - 1.0)\n",
    "        \n",
    "    return - beta * S\n",
    "\n",
    "S_donny_py = []\n",
    "for i in range(len(ens)):\n",
    "    S_donny_py.append(action_donny(phi[i].numpy()))\n",
    "\n",
    "S_donny_py = torch.tensor(S_donny_py)\n",
    "\n",
    "\n",
    "################################################################################################################################################\n",
    "\n",
    "# n = 2\n",
    "# sigma_y = torch.tensor([[0,1j],[-1j,0]]) # VS overall - sign! To discuss\n",
    "# T = (torch.eye(2*n + 2) + torch.block_diag(*[sigma_y for _ in range(n+1)])).cdouble()\n",
    "\n",
    "# S_donny = torch.zeros(phi.shape[0],dtype=torch.cdouble)\n",
    "\n",
    "# for mu in [-3,-4]:\n",
    "#     z_fwd = ( phi.cdouble().transpose(-1,-2) @ (T @ torch.roll(phi.cdouble(),-1,dims=mu) ) ).squeeze(-1,-2) \n",
    "#     zbar_fwd = ( torch.roll(phi.cdouble(),-1,dims=mu).transpose(-1,-2) @ (T @ phi.cdouble() ) ).squeeze(-1,-2) \n",
    "\n",
    "#     S_donny += -beta*(z_fwd * zbar_fwd - 1.0).sum(dim=(-1,-2)) # trivial shift, zero centered\n",
    "\n",
    "print(f\"max(|S_donny_py - S_donny|) = {np.abs(S_donny_py - S_donny).max()}\")\n",
    "print(f\"max(|S_donny_py - S_tej_py|) = {np.abs(S_donny_py - S_tej_py).max()}\")\n",
    "print(f\"max(|S_donny - S_tej_py|) = {np.abs(S_donny - S_tej_py).max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=1,nrows=3,figsize=(15,10))\n",
    "ax[0].plot(S_tej_py,label='tej_py')\n",
    "ax[0].plot(S_donny.real,color='red',label='donny')\n",
    "ax[0].plot(S_tej.real,color='green',label='tej c++')\n",
    "ax[0].legend()\n",
    "ax[0].set_title(\"action values\")\n",
    "ax[1].plot(S_tej_py-S_donny.real)\n",
    "ax[1].set_title(\"diff tej_py - donny\")\n",
    "ax[2].plot(S_tej-S_donny.numpy())\n",
    "ax[2].set_title(\"diff tej_c++ - donny / tej_py\")\n",
    "plt.suptitle(\"comparison action values with Tej's python code\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atol = 1e-12\n",
    "\n",
    "print(\"comparing ens and phi\")\n",
    "print('='*21,'\\n')\n",
    "print(\"comparing real part\")\n",
    "print('-'*19,'\\n')\n",
    "print(f\"Ex: ens[0,0,0].real : {ens[0,0,0].real}\")\n",
    "print(f\"Ex: phi[0,0,0].real: {phi[0,0,0][::2].squeeze(-1).numpy()}\")\n",
    "print(f\"(ens-phi)[0,0,0].real: {ens[0,0,0].real - phi[0,0,0][::2].squeeze(-1).numpy()}\")\n",
    "print(f\"max overall diff.real: {(ens.real - phi[:,:,:,::2].squeeze(-1).numpy()).max()}\")\n",
    "print(f\"diff < {atol}? {np.allclose(ens.real - phi[:,:,:,::2].squeeze(-1).numpy(),np.zeros_like(ens.real),atol=atol)}\")\n",
    "print(\"\")\n",
    "print(\"comparing imag part\")\n",
    "print('-'*19,'\\n')\n",
    "print(f\"Ex: ens[0,0,0].imag : {ens[0,0,0].imag}\")\n",
    "print(f\"Ex: phi[0,0,0].imag: {phi[0,0,0][1::2].squeeze(-1).numpy()}\")\n",
    "print(f\"(ens-phi)[0,0,0].imag: {ens[0,0,0].imag - phi[0,0,0][1::2].squeeze(-1).numpy()}\")\n",
    "print(f\"max overall diff.imag: {(ens.imag - phi[:,:,:,1::2].squeeze(-1).numpy()).max()}\")\n",
    "print(f\"diff < {atol}? {np.allclose(ens.imag - phi[:,:,:,1::2].squeeze(-1).numpy(),np.zeros_like(ens.imag),atol=atol)}\")\n",
    "print(\"\")\n",
    "print(\"comparing real2cmplx(cmplx2real(ens)) with ens\")\n",
    "print('-'*46,'\\n')\n",
    "z, _ = real2cmplx(cmplx2real(torch.tensor(ens).unsqueeze(-1)))\n",
    "z = z.squeeze(-1).numpy()\n",
    "print(\"z ~ real2cmplx(cmplx2real(ens))\\n\")\n",
    "print(f\"(z - ens).real < {atol}? {np.allclose((z - ens).real,np.zeros_like(ens.real),atol=atol)}\")\n",
    "print(f\"(z - ens).imag < {atol}? {np.allclose((z - ens).imag,np.zeros_like(ens.imag),atol=atol)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(u,label='Tej')\n",
    "plt.plot(u_donny, label='Donny',color='red')\n",
    "plt.title(r\"$u(\\phi)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(u - u_donny.numpy())\n",
    "plt.title(r\"$\\Delta u(\\phi)$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(S_tej,label='Tej')\n",
    "plt.plot(S_donny, label='Donny',color='red')\n",
    "plt.title(r\"$S(\\phi)$\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15,5))\n",
    "plt.plot(S_tej - S_donny.numpy())\n",
    "plt.title(r\"$\\Delta S(\\phi)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing fuzzy 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *\n",
    "\n",
    "from torch.cuda.amp import autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_tilde, detJ = deformation.complexify(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 64\n",
    "beta = 4.5\n",
    "Nc = 3\n",
    "n_cfg = 1_000\n",
    "print(\"Reading samples..\\n\")\n",
    "ens = np.fromfile(f'cpn_b{beta:.1f}_L{L}_Nc{Nc}_ens.dat', dtype=np.complex128).reshape(n_cfg, L, L, Nc)\n",
    "print(\"...done\\n\")\n",
    "\n",
    "# SAMPLES\n",
    "n=2\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "\n",
    "print(\"Preparing samples..\\n\")\n",
    "phi = cmplx2real(torch.tensor(ens).unsqueeze(-1))\n",
    "print(\"...done\\n\")\n",
    "print(f\"{phi.shape = }\")\n",
    "print(f\"{phi.dtype = }\")\n",
    "\n",
    "# ACTION FUNCTIONAL\n",
    "S = lambda phi: LatticeActionFunctional(n).action_centered(phi.cdouble(),beta)\n",
    "\n",
    "# OBSERVABLE\n",
    "i,j = 0, 1 \n",
    "obs = LatObs.fuzzy_one\n",
    "# obs = lambda phi: LatObs.one_pt(phi,i,j) # fuzzy_zero\n",
    "# obs = lambda phi: LatObs.two_pt(phi,i,j)\n",
    "\n",
    "a0 = 0.001 * torch.rand(L,L,dim)\n",
    "deformation = Homogeneous(a0,n,mode=\"2D\")\n",
    "\n",
    "deformation_type = \"lattice\"\n",
    "\n",
    "batch_size = 16\n",
    "alpha = 1e-4\n",
    "\n",
    "# LOSS\n",
    "loss_fct = loss\n",
    "loss_name = 'loss' if loss_fct == loss else 'logloss'\n",
    "\n",
    "# MODEL\n",
    "params = dict(\n",
    "    action = S,\n",
    "    deformation = deformation,\n",
    "    observable = obs,\n",
    "    beta = beta\n",
    ")\n",
    "model = CP(n,**params)\n",
    "\n",
    "# SET EPOCHS\n",
    "epochs = 10\n",
    "\n",
    "# TRAINING\n",
    "print(\"\\n training model ... \\n\")\n",
    "\n",
    "observable, observable_var, losses_train, losses_val, anorm, a0, af = train(model,phi,epochs=epochs,loss_fct=loss_fct,batch_size=batch_size,lr=alpha)\n",
    "\n",
    "print(\"\\n done.\\n\")\n",
    "\n",
    "undeformed_obs = obs(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import analysis as al\n",
    "import seaborn as sns\n",
    "\n",
    "epochs = len(observable)\n",
    "\n",
    "\n",
    "Nboot = 100\n",
    "mean_re, err_re = al.bootstrap(grab(undeformed_obs),Nboot=Nboot,f=al.rmean)\n",
    "mean_im, err_im = al.bootstrap(grab(undeformed_obs),Nboot=Nboot,f=al.imean)\n",
    "\n",
    "\n",
    "# LINEAR DEFORMATION\n",
    "if deformation_type == \"linear\":\n",
    "    aZ = torch.tensor(af[0]).cfloat()\n",
    "    aW = torch.tensor(af[1]).cfloat()\n",
    "elif deformation_type == \"homogeneous\":\n",
    "# HOMOGENEOUS DEFORMATION\n",
    "    su_n = LieSU(n+1)\n",
    "    aZ = su_n.embed(torch.tensor(af[0]))\n",
    "    aW = su_n.embed(torch.tensor(af[1]))\n",
    "else:\n",
    "    su_n = LieSU(n+1)\n",
    "    aZ = su_n.embed(torch.tensor(af[0,0]))\n",
    "    aW = su_n.embed(torch.tensor(af[0,1]))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4,ncols=2,figsize=(15,10),gridspec_kw={'height_ratios': [1, 1, 2, 2]})\n",
    "\n",
    "ax[0,1].plot(anorm)\n",
    "ax[0,1].set_title('norm a')\n",
    "\n",
    "ax[0,0].plot(losses_train,label='loss')\n",
    "ax[0,0].plot(losses_val,label='val_loss')\n",
    "ax[0,0].legend()\n",
    "ax[0,0].set_title(loss_name)\n",
    "ax[0,0].legend()\n",
    "\n",
    "ax[1,0].plot([z.real for z in observable],label='re')\n",
    "ax[1,0].plot([z.imag for z in observable],label='im',color='purple')\n",
    "ax[1,0].axhline(y=mean_re,xmin=0,xmax=epochs,label='OG re',color='red')\n",
    "ax[1,0].axhline(y=mean_im,xmin=0,xmax=epochs,label='OG im',color='orange')\n",
    "ax[1,0].fill_between([0,epochs], [mean_re-err_re]*2, [mean_re+err_re]*2, alpha=0.5, color='red')\n",
    "ax[1,0].fill_between([0,epochs], [mean_im-err_im]*2, [mean_im+err_im]*2, alpha=0.5, color='orange')\n",
    "ax[1,0].set_title('defromed obs')\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[1,1].plot([z.real for z in observable_var],label='deformed')\n",
    "ax[1,1].axhline(y=undeformed_obs.var(),xmin=0,xmax=epochs,label='undeformed',color='red')\n",
    "ax[1,1].set_title('obs variance')\n",
    "ax[1,1].legend()\n",
    "\n",
    "sns.heatmap(data=aZ.real,ax=ax[2,0],cmap='coolwarm',vmin=-0.1,vmax=0.1)\n",
    "ax[2,0].set_title('real(a[0]) after training')\n",
    "sns.heatmap(data=aZ.imag,ax=ax[2,1],cmap='coolwarm',vmin=-0.1,vmax=0.1)\n",
    "ax[2,1].set_title('imag(a[0]) after training')\n",
    "\n",
    "sns.heatmap(data=aW.real,ax=ax[3,0],cmap='coolwarm',vmin=-0.1,vmax=0.1)\n",
    "ax[3,0].set_title('real(a[1]) after training')\n",
    "sns.heatmap(data=aW.imag,ax=ax[3,1],cmap='coolwarm',vmin=-0.1,vmax=0.1)\n",
    "ax[3,1].set_title('imag(a[1]) after training')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Lattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *\n",
    "\n",
    "import analysis as al"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(n,observable,observable_var,undeformed_obs,af,anorm,losses_train,losses_val,loss_name,deformation_type,title=None):\n",
    "    # VARIANCE PLOT\n",
    "    epochs = len(observable)\n",
    "    \n",
    "\n",
    "    Nboot = 1000\n",
    "    mean_re, err_re = al.bootstrap(grab(undeformed_obs),Nboot=Nboot,f=al.rmean)\n",
    "    mean_im, err_im = al.bootstrap(grab(undeformed_obs),Nboot=Nboot,f=al.imean)\n",
    "\n",
    "\n",
    "    # LINEAR DEFORMATION\n",
    "    if deformation_type == \"Linear\":\n",
    "        aZ = torch.tensor(af[0]).cfloat()\n",
    "        aW = torch.tensor(af[1]).cfloat()\n",
    "    elif deformation_type == \"Homogeneous\":\n",
    "    # HOMOGENEOUS DEFORMATION\n",
    "        su_n = LieSU(n+1)\n",
    "        aZ = su_n.embed(torch.tensor(af[0]))\n",
    "        aW = su_n.embed(torch.tensor(af[1]))\n",
    "    else:\n",
    "        su_n = LieSU(n+1)\n",
    "        aZ = su_n.embed(torch.tensor(af[0,0]))\n",
    "        aW = su_n.embed(torch.tensor(af[0,1]))\n",
    "\n",
    "\n",
    "    # OBSERVABLE\n",
    "    fig, ax = plt.subplots(nrows=2,ncols=2) #[1, 1, 2, 2] gridspec_kw={'height_ratios': [1,1]}\n",
    "\n",
    "    ax[0,1].plot(anorm)\n",
    "    ax[0,1].set_title('norm a')\n",
    "\n",
    "    ax[0,0].plot(losses_train,label='loss')\n",
    "    ax[0,0].plot(losses_val,label='val_loss')\n",
    "    ax[0,0].legend()\n",
    "    ax[0,0].set_title(loss_name)\n",
    "    ax[0,0].legend()\n",
    "\n",
    "    ax[1,0].plot([z.real for z in observable],label='re')\n",
    "    # ax[1,0].plot([z.imag for z in observable],label='im',color='purple')\n",
    "    # ax[1,0].plot([z for z in observable],label='re') # full 2pt fct\n",
    "    ax[1,0].axhline(y=mean_re,xmin=0,xmax=epochs,label='OG re',color='red')\n",
    "    # ax[1,0].axhline(y=mean_im,xmin=0,xmax=epochs,label='OG im',color='orange')\n",
    "    ax[1,0].fill_between([-100,epochs], [mean_re-err_re]*2, [mean_re+err_re]*2, alpha=0.5, color='red')\n",
    "    # ax[1,0].fill_between([-100,epochs], [mean_im-err_im]*2, [mean_im+err_im]*2, alpha=0.5, color='orange')\n",
    "    ax[1,0].set_title('defromed obs')\n",
    "    ax[1,0].legend()\n",
    "\n",
    "    ax[1,1].plot([z.real for z in observable_var],label='deformed')\n",
    "    ax[1,1].axhline(y=undeformed_obs.var(),xmin=0,xmax=epochs,label='undeformed',color='red')\n",
    "    ax[1,1].set_title('obs variance')\n",
    "    ax[1,1].legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout();\n",
    "\n",
    "    # LEARNED DEFORMATION PARAMETER\n",
    "    fig, ax = plt.subplots(nrows=2,ncols=2)\n",
    "\n",
    "    sns.heatmap(data=aZ.real,ax=ax[0,0],cmap='coolwarm')\n",
    "    ax[0,0].set_title('real(a[0]) after training')\n",
    "    sns.heatmap(data=aZ.imag,ax=ax[0,1],cmap='coolwarm')\n",
    "    ax[0,1].set_title('imag(a[0]) after training')\n",
    "\n",
    "    sns.heatmap(data=aW.real,ax=ax[1,0],cmap='coolwarm')\n",
    "    ax[1,0].set_title('real(a[1]) after training')\n",
    "    sns.heatmap(data=aW.imag,ax=ax[1,1],cmap='coolwarm')\n",
    "    ax[1,1].set_title('imag(a[1]) after training')\n",
    "\n",
    "    plt.suptitle(title + \" deformation params\")\n",
    "    plt.tight_layout()\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################ LATTICE ########################\n",
    "L = 64\n",
    "beta = 4.5\n",
    "Nc = 3\n",
    "n_cfg = 1_000\n",
    "ens = np.fromfile(f'cpn_b{beta:.1f}_L{L}_Nc{Nc}_ens.dat', dtype=np.complex128).reshape(n_cfg, L, L, Nc)\n",
    "\n",
    "\n",
    "# PREPPING SAMPLES\n",
    "n=2\n",
    "rk = n\n",
    "dim = n**2 + 2*n\n",
    "phi = cmplx2real(torch.tensor(ens).unsqueeze(-1))\n",
    "print(f\"{phi.shape = }\")\n",
    "print(f\"{phi.dtype = }\")\n",
    "\n",
    "# ACTION FUNCTIONAL\n",
    "S = lambda phi: LatticeActionFunctional(n).action_centered(phi.cdouble(),beta)\n",
    "\n",
    "# OBSERVABLE\n",
    "i,j = 0, 1 \n",
    "obs = LatObs.fuzzy_one\n",
    "# obs = lambda phi: LatObs.one_pt(phi,i,j) # fuzzy_zero\n",
    "# obs = lambda phi: LatObs.two_pt(phi,i,j)\n",
    "\n",
    "a0 = 1e-5 * torch.rand(L,L,dim)\n",
    "deformation = Homogeneous(a0,n,spacetime=\"2D\")\n",
    "\n",
    "deformation_type = \"lattice\"\n",
    "batch_size = 16\n",
    "\n",
    "# LEARNING RATE \n",
    "alpha = 1e-4\n",
    "\n",
    "# LOSS\n",
    "loss_fct = rloss\n",
    "\n",
    "# MODEL\n",
    "params = dict(\n",
    "    action = S,\n",
    "    deformation = deformation,\n",
    "    observable = obs,\n",
    "    beta = beta\n",
    ")\n",
    "\n",
    "model = CP(n,**params)\n",
    "\n",
    "# SET EPOCHS\n",
    "epochs = 100\n",
    "\n",
    "# TRAINING\n",
    "observable, observable_var, losses_train, losses_val, anorm, a0, af = train(model,phi,epochs=epochs,loss_fct=loss_fct,batch_size=batch_size,lr=alpha)\n",
    "\n",
    "undeformed_obs = obs(phi)\n",
    "\n",
    "plot_data(n,observable,observable_var,undeformed_obs,af,anorm,losses_train,losses_val,loss_fct.__name__,deformation_type,obs.__name__)#loss_name[loss_fct]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from scipy.linalg import block_diag\n",
    "from scipy.special import binom\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "from analysis import bootstrap\n",
    "\n",
    "from typing import Callable\n",
    "from numpy.typing import ArrayLike\n",
    "\n",
    "from mcmc import *\n",
    "from model import *\n",
    "from deformations import *\n",
    "from losses import *\n",
    "from linalg import *\n",
    "from observables import *\n",
    "from unet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "Lx = 64\n",
    "Ly = 64\n",
    "dim_g = n**2 + 2*n\n",
    "\n",
    "unet = UNET(n,Lx,Ly)\n",
    "# a = torch.randn(dim_g,Lx,Ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0201, -0.0239, -0.0243,  ..., -0.0239, -0.0239, -0.0272],\n",
       "         [-0.0216, -0.0277, -0.0284,  ..., -0.0273, -0.0268, -0.0302],\n",
       "         [-0.0211, -0.0277, -0.0285,  ..., -0.0274, -0.0272, -0.0301],\n",
       "         ...,\n",
       "         [-0.0214, -0.0267, -0.0281,  ..., -0.0271, -0.0266, -0.0298],\n",
       "         [-0.0220, -0.0279, -0.0288,  ..., -0.0278, -0.0277, -0.0304],\n",
       "         [-0.0217, -0.0271, -0.0282,  ..., -0.0279, -0.0276, -0.0305]],\n",
       "\n",
       "        [[-0.0323, -0.0311, -0.0315,  ..., -0.0319, -0.0317, -0.0364],\n",
       "         [-0.0291, -0.0241, -0.0242,  ..., -0.0247, -0.0252, -0.0273],\n",
       "         [-0.0296, -0.0245, -0.0249,  ..., -0.0250, -0.0257, -0.0281],\n",
       "         ...,\n",
       "         [-0.0297, -0.0253, -0.0244,  ..., -0.0255, -0.0263, -0.0271],\n",
       "         [-0.0303, -0.0261, -0.0258,  ..., -0.0258, -0.0269, -0.0279],\n",
       "         [-0.0353, -0.0310, -0.0303,  ..., -0.0305, -0.0309, -0.0288]],\n",
       "\n",
       "        [[ 0.0116,  0.0120,  0.0133,  ...,  0.0126,  0.0138,  0.0079],\n",
       "         [ 0.0042,  0.0067,  0.0084,  ...,  0.0087,  0.0080,  0.0059],\n",
       "         [ 0.0045,  0.0052,  0.0075,  ...,  0.0063,  0.0071,  0.0046],\n",
       "         ...,\n",
       "         [ 0.0040,  0.0054,  0.0061,  ...,  0.0063,  0.0056,  0.0044],\n",
       "         [ 0.0050,  0.0043,  0.0062,  ...,  0.0054,  0.0057,  0.0027],\n",
       "         [-0.0062, -0.0045, -0.0041,  ..., -0.0033, -0.0039, -0.0008]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0314, -0.0273, -0.0241,  ..., -0.0242, -0.0241, -0.0267],\n",
       "         [-0.0280, -0.0253, -0.0235,  ..., -0.0241, -0.0256, -0.0281],\n",
       "         [-0.0279, -0.0265, -0.0244,  ..., -0.0249, -0.0263, -0.0286],\n",
       "         ...,\n",
       "         [-0.0282, -0.0263, -0.0261,  ..., -0.0257, -0.0278, -0.0293],\n",
       "         [-0.0283, -0.0274, -0.0275,  ..., -0.0275, -0.0284, -0.0307],\n",
       "         [-0.0254, -0.0277, -0.0284,  ..., -0.0279, -0.0296, -0.0296]],\n",
       "\n",
       "        [[ 0.0355,  0.0287,  0.0281,  ...,  0.0281,  0.0293,  0.0247],\n",
       "         [ 0.0325,  0.0263,  0.0263,  ...,  0.0261,  0.0256,  0.0256],\n",
       "         [ 0.0327,  0.0280,  0.0282,  ...,  0.0284,  0.0279,  0.0284],\n",
       "         ...,\n",
       "         [ 0.0322,  0.0268,  0.0277,  ...,  0.0273,  0.0266,  0.0282],\n",
       "         [ 0.0314,  0.0275,  0.0282,  ...,  0.0281,  0.0273,  0.0295],\n",
       "         [ 0.0292,  0.0247,  0.0266,  ...,  0.0257,  0.0246,  0.0249]],\n",
       "\n",
       "        [[ 0.0172,  0.0093,  0.0104,  ...,  0.0104,  0.0108,  0.0098],\n",
       "         [ 0.0201,  0.0116,  0.0111,  ...,  0.0114,  0.0113,  0.0131],\n",
       "         [ 0.0194,  0.0097,  0.0107,  ...,  0.0099,  0.0112,  0.0117],\n",
       "         ...,\n",
       "         [ 0.0194,  0.0104,  0.0106,  ...,  0.0109,  0.0110,  0.0129],\n",
       "         [ 0.0185,  0.0087,  0.0093,  ...,  0.0085,  0.0095,  0.0117],\n",
       "         [ 0.0202,  0.0108,  0.0106,  ...,  0.0103,  0.0109,  0.0143]]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISC & OTHER TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING BLOCK DIAG MATRICES WITH BATCHING blkdiag\n",
    "a = torch.randint(low=0,high=9,size=(1,3,2,2))\n",
    "b = torch.randint(low=0,high=9,size=(1,3,2,2))\n",
    "c = torch.randint(low=0,high=9,size=(1,3,2,2))\n",
    "\n",
    "print(f'{a.shape = }\\n')\n",
    "arrs = [a,b,c]\n",
    "shapes = np.array([i.shape for i in arrs])\n",
    "\n",
    "print(f'{shapes = }\\n')\n",
    "\n",
    "print(f'{shapes[0,:-2] = }\\n')\n",
    "\n",
    "print(f'{list(shapes[0,:-2])+[shapes[:, -1].sum(), shapes[:,-2].sum()]}\\n')\n",
    "\n",
    "# np.zeros([shapes[:, i].sum() for i in range(len(arrs))])\n",
    "\n",
    "# print(f'{a = }\\n')\n",
    "# print(f'{b = }\\n')\n",
    "# print(f'{c = }\\n')\n",
    "M = blkdiag(arrs)\n",
    "print(f'{M.shape = }\\n')\n",
    "print(M)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING S : REAL VS CMPLX\n",
    "\n",
    "# REAL SETUP\n",
    "n = 3 # complex dimension of CP^n\n",
    "\n",
    "diff_re, diff_im, S_real_im, S_cmplx_im = [], [], [], []\n",
    "\n",
    "for i in range(100):\n",
    "    Z = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "    Z/= np.linalg.norm(Z)\n",
    "\n",
    "    W = np.random.normal(loc=0,scale=1,size = (2*n + 2)).reshape(-1,1)\n",
    "    W /= np.linalg.norm(W)\n",
    "\n",
    "\n",
    "    # COMPARISON WITH COMPLEX ACTION\n",
    "    reZ = Z[0::2]\n",
    "    imZ = Z[1::2]\n",
    "\n",
    "    reW = W[0::2]\n",
    "    imW = W[1::2]\n",
    "\n",
    "    z = np.array([a + b*1j for a,b in zip(reZ,imZ)])\n",
    "    w = np.array([a + b*1j for a,b in zip(reW,imW)])\n",
    "\n",
    "    # DIM NEEDED FOR S (samples,2n+2,1)\n",
    "    Z = Z[np.newaxis,:,:]\n",
    "    W = W[np.newaxis,:,:]\n",
    "\n",
    "    # print(f'{S(n,Z,W,beta=1) = }')\n",
    "    # print(f'{Scmplx(z.flatten(),w.flatten(),beta=1) = }')\n",
    "    diff_re.append((S(n,Z,W,beta=1) - Scmplx(z.flatten(),w.flatten(),beta=1)).real)\n",
    "    diff_im.append((S(n,Z,W,beta=1) - Scmplx(z.flatten(),w.flatten(),beta=1)).imag)\n",
    "    S_real_im.append(S(n,Z,W,beta=1).imag)\n",
    "    S_cmplx_im.append(Scmplx(z.flatten(),w.flatten(),beta=1).imag)\n",
    "\n",
    "# print(f'S - Scmpl = {(S(n,Z,W,beta=1) - Scmplx(z.flatten(),w.flatten(),beta=1)).real}')\n",
    "names = ['diff_re','diff_im','S_real_im','S_cmplx_im']\n",
    "i = 0\n",
    "for f in [diff_re,diff_im,S_real_im,S_cmplx_im]:\n",
    "    plt.plot(f,label=names[i])\n",
    "    i+=1\n",
    "\n",
    "plt.title('im(S) and S-Scmplx')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.vonmises(0.0,2,size=10_000)\n",
    "plt.hist(data,bins=30)\n",
    "plt.title('angles drawn from von Mises diatr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING DIFFS\n",
    "x = np.array([1,0,0])\n",
    "y = np.array([0,1,0])\n",
    "\n",
    "u = np.array([1,0,0])\n",
    "v = np.array([0,1,0])\n",
    "\n",
    "z = 1 + 2j\n",
    "\n",
    "x_prime,y_prime = invPsi(Psi((x,y)))\n",
    "print(f'{x_prime - x = }')\n",
    "print(f'{y_prime - y = }')\n",
    "\n",
    "w = Psi(invPsi(z))\n",
    "print(f'{w - z = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
